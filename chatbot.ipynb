{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações de Bibliotecas\n",
    "Carrega as bibliotecas necessárias para manipulação de dados, modelagem de redes neurais, tokenização e processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pichau\\Desktop\\codes\\desafio cientista de dados cnpq\\chatbot_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação do Dispositivo\n",
    "Define o dispositivo (device) para usar GPU (cuda) ou CPU, dependendo da disponibilidade, garantindo uma execução mais rápida quando a GPU estiver disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração do Modelo (Config Class)\n",
    "Define os hiperparâmetros e configurações do modelo, como o número de classes (intenção), épocas de treinamento, tamanho do lote, taxa de aprendizado e comprimento máximo dos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    num_classes = 10  # Ajuste com base no número de intenções\n",
    "    epochs = 15\n",
    "    batch_size = 8\n",
    "    lr = 1e-5\n",
    "    max_length = 15\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregamento e Extração do Dataset JSON\n",
    "Carrega o arquivo intent.json, que contém as intenções, padrões de texto e respostas. Extrai as primeiras dez intenções para limitar o escopo do modelo, organizando textos, labels e respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intent.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_intents = data['intents'][:cfg.num_classes]  # Usar apenas as primeiras 10 intenções\n",
    "texts = []\n",
    "labels = []\n",
    "responses = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapeamento de Intenções para Labels\n",
    "Cria dicionários (intent2label e label2intent) para mapear cada intenção para um índice numérico, facilitando o uso do modelo para classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent2label = {}\n",
    "label2intent = {}\n",
    "for idx, intent_data in enumerate(df_intents):\n",
    "    intent = intent_data['intent']\n",
    "    intent2label[intent] = idx\n",
    "    label2intent[idx] = intent\n",
    "    for text in intent_data['text']:\n",
    "        texts.append(text)\n",
    "        labels.append(idx)\n",
    "    # Coletar respostas para cada intenção\n",
    "    responses[intent] = intent_data['responses']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação dos Dados\n",
    "Gera um DataFrame para visualizar e manipular dados, incluindo texto e seus respectivos rótulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'text': texts, 'label': labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização com BertTokenizer\n",
    "Prepara o texto para o modelo BERT, convertendo-o em uma sequência de IDs com padding e truncamento, conforme o comprimento máximo definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição da Classe IntentDataset\n",
    "Define uma classe Dataset personalizada para carregar textos e labels tokenizados, garantindo compatibilidade com DataLoader do PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.encodings = tokenizer(texts, padding='max_length', max_length=cfg.max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divisão do Conjunto de Dados\n",
    "Divide os dados em conjuntos de treinamento e validação, utilizando uma proporção de 80% para treino e 20% para validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de Objetos Dataset\n",
    "Cria os datasets de treino e validação para serem utilizados pelo DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IntentDataset(train_texts, train_labels)\n",
    "val_dataset = IntentDataset(val_texts, val_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do Modelo BertClassifier\n",
    "Define o modelo de classificação com BertModel, seguido por uma camada linear para identificar as intenções. Inclui Dropout para regularização e ReLU como função de ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, cfg.num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # Usar o pooler_output\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        output = self.linear(dropout_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação e Carregamento do Modelo Salvo\n",
    "Verifica se o modelo salvo (intent_model.pth) existe. Se existir, carrega o modelo, caso contrário, inicia o treinamento e salva o modelo para futuras execuções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo salvo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_12756\\2633590380.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'intent_model.pth'\n",
    "model = BertClassifier()\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Carregando modelo salvo...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "else:\n",
    "    print(\"Modelo não encontrado. Iniciando o treinamento...\")\n",
    "    train(model, train_dataset, val_dataset, cfg.lr, cfg.epochs)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Modelo treinado e salvo.\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função de Treinamento (train)\n",
    "Define a função de treinamento para calcular a perda e a precisão dos conjuntos de treino e validação. Utiliza CrossEntropyLoss e o otimizador Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset, learning_rate, epochs):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss_train += loss.item()\n",
    "            acc = (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss_val += loss.item()\n",
    "                acc = (outputs.argmax(dim=1) == labels).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "        print(f\"Train Loss: {total_loss_train / len(train_dataset):.3f}, Train Accuracy: {total_acc_train / len(train_dataset):.3f}\")\n",
    "        print(f\"Val Loss: {total_loss_val / len(val_dataset):.3f}, Val Accuracy: {total_acc_val / len(val_dataset):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função de Predição (predict_intent)\n",
    "Usa o modelo para prever a intenção de uma frase de entrada. Retorna a intenção com maior confiança, caso seja superior ao threshold definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(model, text):\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer(text, return_tensors='pt', padding='max_length', max_length=cfg.max_length, truncation=True)\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted_label = torch.max(probs, dim=1)\n",
    "        if confidence.item() < 0.3:\n",
    "            return None\n",
    "        intent = label2intent[predicted_label.item()]\n",
    "        return intent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalização da Resposta (personalize_response)\n",
    "Substitui <HUMAN> pelo nome do usuário para personalizar a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalize_response(response, user_name):\n",
    "    return response.replace(\"<HUMAN>\", user_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcionalidade Principal do Chatbot (main)\n",
    "Gera uma interface interativa, pede o nome do usuário, permite a consulta das últimas três intenções detectadas e exibe respostas personalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá! Eu sou o seu assistente virtual.\n",
      "Prazer em conhecê-lo, Nathan!\n",
      "Nathan, sua intenção é: CourtesyGreeting\n",
      "Resposta: Hi, I am great, how are you? Please tell me your GeniSys user\n",
      "Nathan, sua intenção é: CourtesyGreetingResponse\n",
      "Resposta: Great! Hi Nathan! How can I help?\n",
      "Nathan, sua intenção é: CurrentHumanQuery\n",
      "Resposta: Nathan, what can I do for you?\n",
      "Últimas três intenções detectadas:\n",
      "1. CurrentHumanQuery\n",
      "2. CourtesyGreetingResponse\n",
      "3. CourtesyGreeting\n",
      "Encerrando a conversa. Até logo!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global model\n",
    "    user_name = \"\"\n",
    "    intents_history = deque(maxlen=3)\n",
    "\n",
    "    print(\"Olá! Eu sou o seu assistente virtual.\")\n",
    "    if not user_name:\n",
    "        user_name = input(\"Por favor, qual é o seu nome? \").strip()\n",
    "        print(f\"Prazer em conhecê-lo, {user_name}!\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"{user_name}: \").strip()\n",
    "        if user_input.lower() in ['sair', 'exit', 'quit']:\n",
    "            print(\"Encerrando a conversa. Até logo!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'consultar intenções':\n",
    "            if intents_history:\n",
    "                print(\"Últimas três intenções detectadas:\")\n",
    "                for idx, intent in enumerate(list(intents_history)[::-1], 1):\n",
    "                    print(f\"{idx}. {intent}\")\n",
    "            else:\n",
    "                print(\"Nenhuma intenção detectada até o momento.\")\n",
    "            continue\n",
    "\n",
    "        intent = predict_intent(model, user_input)\n",
    "        if intent:\n",
    "            intents_history.append(intent)\n",
    "            response = random.choice(responses[intent])\n",
    "            personalized_response = personalize_response(response, user_name)\n",
    "            print(f\"{user_name}, sua intenção é: {intent}\")\n",
    "            print(f\"Resposta: {personalized_response}\")\n",
    "        else:\n",
    "            print(f\"Desculpe, {user_name}, não consegui identificar sua intenção.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados e Casos de Teste\n",
    "Resultados: O chatbot identifica intenções e responde de maneira personalizada, armazenando as últimas três intenções detectadas. Responde com o nome do usuário ao substituir <HUMAN> na resposta.\n",
    "\n",
    "## Casos de Teste:\n",
    "\n",
    "- Entrada: \"How are you?\"\n",
    "- Saída Esperada: \"Hello, I am great, how are you, [Nome]?\"\n",
    "\n",
    "## Respostas Aleatórias:\n",
    "- Entrada: \"Hi\"\n",
    "- Saída Esperada: \"Hi, please tell me your GeniSys user.\"\n",
    "\n",
    "## Consulta de Intenções:\n",
    "\n",
    "- Entrada: \"consultar intenções\"\n",
    "- Saída Esperada: Lista das últimas três intenções detectadas.\n",
    "\n",
    "## Predição do Nome:\n",
    "- Entrada: \"What is my name?\"\n",
    "- Saída Esperada: \"They call you [Nome], what can I do for you?\"\n",
    "\n",
    "## Encerramento:\n",
    "\n",
    "- Entrada: \"sair\"\n",
    "- Saída Esperada: \"Encerrando a conversa. Até logo!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
